<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks
              for Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wuxiyang1996.github.io/" target="_blank">Xiyang Wu</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://tianruiguan.phd/" target="_blank">Tianrui Guan</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://theorg.com/org/citadel-securities/org-chart/dianqi-li" target="_blank">Dianqi Li</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://shuaiyihuang.github.io/" target="_blank">Shuaiyi Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/xiaoyuliu1231/" target="_blank">Xiaoyu Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://xijun-cs.github.io/" target="_blank">Xijun Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://ricky-xian.github.io/" target="_blank">Ruiqi Xian</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.umd.edu/~abhinav/" target="_blank">Abhinav Shrivastava</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://furong-huang.com/" target="_blank">Furong Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://users.umiacs.umd.edu/~jbg/" target="_blank">Jordan Lee Boyd-Graber</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://tianyizhou.github.io/" target="_blank">Tianyi Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a><sup>1</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Maryland, College Park<sup>1</sup>,
                      Citadel Securities<sup>2</sup><br>
                      <sup>*</sup>Equal Contribution</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.10900.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wuxiyang1996/AutoHallusion" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- First Image with adjusted size and centered -->
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/teaser.png" alt="Teaser Image" style="width: 1000px; display: block; margin: auto;"> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
        <div style="max-width: 800px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-justified">
            <br>
            AUTOHALLUSION is the first automatic benchmark generation approach that harnesses a few principal
            strategies to create diverse hallucination examples by probing the language modules in LVLMs for context cues.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Large vision-language models (LVLMs) hallucinate: certain context cues in an image may trigger the language
            module's overconfident and incorrect reasoning on abnormal or hypothetical objects. Though a few benchmarks
            have been developed to investigate LVLM hallucinations, they mainly rely on hand-crafted corner cases whose
            fail patterns may hardly generalize, and finetuning on them could undermine their validity. These motivate
            us to develop the first automatic benchmark generation approach, AUTOHALLUSION, that harnesses a few
            principal strategies to create diverse hallucination examples. It probes the language modules in LVLMs for
            context cues and uses them to synthesize images by: (1) adding objects abnormal to the context cues; (2)
            for two co-occurring objects, keeping one and excluding the other; or (3) removing objects closely tied to
            the context cues. It then generates image-based questions whose ground-truth answers contradict the language
            module's prior. A model has to overcome contextual biases and distractions to reach correct answers, while
            incorrect or inconsistent answers indicate hallucinations. AUTOHALLUSION enables us to create new benchmarks
            at the minimum cost and thus overcomes the fragility of hand-crafted benchmarks. It also reveals common
            failure patterns and reasons, providing key insights to detect, avoid, or control hallucinations.
            Comprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5,
            show a 97.7% and 98.7% success rate of hallucination induction on synthetic and real-world datasets of
            AUTOHALLUSION, paving the way for a long battle against hallucinations.
<!--          <br>-->
<!--            <br>-->
<!--            <b>TL, DR:</b> We develop the first automatic benchmark generation approach that harnesses a few principal-->
<!--            strategies to create diverse hallucination examples by probing the language modules in LVLMs for context cues.-->
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">The Problem</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
            <source src="static/videos/problem.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        The use of LLMs/VLMs has revolutionized how we interact with robots,
        offering unprecedented levels of understanding and responsiveness. But at what cost?
      </div>
    </div>
  </div>
</section>


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">The Method</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
            <source src="static/videos/method.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        We uncover how these advancements while being impressive, expose robotic systems to even simple
        adversarial attacks, threatening their reliability and safety.
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">The Result</h2>
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/method.png" alt="Teaser Image" style="width: 1000px; display: block; margin: auto;"> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
      </div>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">

        <b>Multi-modal Attacks to LLMs/VLMs in Robotic Applications.</b> The middle pipeline is an abstract robotic
        system with LLMs/VLMs, and multi-modal attacks are applied at visual and text prompts. The left-hand side provides
        different attacks to images, such as reducing image quality, applying transformation, and adding new objects.
        The right-hand side shows different types of attacks in text, including simple rephrasing, stealth rephrasing,
        extension rephrasing, and rephrasing of adjectives and nouns.
      </div>
    </div>
  </div>
</section>


<section class="hero is-small-light">
  <div class="hero-body">
    <div class="container">
    <h2 class="title is-3">The Crafted Hallusion Showcases</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <br>
       <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <b class="title is-4">Abnormal Object Insertion – Existence </b>
          </div>
        <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <b>Scene: </b> <i>Messy office workspace</i>
         <br>
        <b>Detected Objects: </b> <i>Office swivel chair</i>, <i>Open laptop computer</i>, <i>Dell computer monitor</i>,
          <i>Office work desk</i>, <i>Black laptop backpack</i>
          <br>
        <b>Inserted Object: </b> <i>Sheep</i>
          <br>
        <b>Image Manipulation: </b> <i>Sheep</i> is inserted to the image.
        </div>
        </div>
      </div>
      </div>
    </div>

  <div class="container is-max-desktop" style="max-width: 1200px; margin: auto;">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" style="padding:0 60px 0 60px;">
        <div class="content">
          <div class="publication-video">
            <img src="static/showcases/insert_1/init.png" alt="Original Image" style="width: 400px; display: block; margin: auto;">
          </div>
          <p align="center">Original Image</p>
          <p align="left">
            <b>Attack: </b> Adding new irrelevant object <i>Sheep</i> – Existence
            <br>
            <b>Question: </b> Is there a sheep in this image?
            <br>
            <b>GPT-4V: </b> <b style="color: red;background:yellow">Yes, there is an image of a sheep that seems to
          have been digitally added onto a chair in the top half of the image.</b> It looks quite out of place in the
          office setting depicted in the photo.
          </p>
          <br>
          <p align="left">
            <b>Attack: </b> Adding new irrelevant object <i>Sheep</i> with image-level caption – Existence
            <br>
            <b>Question: </b> We have an image depicting This image shows a contemporary office workspace featuring
            several desks cluttered with technological and office equipment. On the left side of the image, there is a
            vivid scene where a brown goat is digitally superimposed and sitting atop a purple office chair. This
            humorous addition contrasts with the otherwise ordinary office setting. The workspace is equipped with
            multiple monitors, laptops, office supplies like pens and staplers, and personal items. The floor is tiled,
            and the overall appearance suggests a busy, creative, and somewhat messy environment typical of tech or
            design-oriented workplaces. Is there a sheep in this image?
            <br>
            <b>GPT-4V: </b> <b style="color: red;background:orange">No, there isn't a sheep in the image.</b>
            What you see is a brown goat that is digitally superimposed and sitting on a purple office chair.
            There are no sheep visible in the picture.
          </p>

        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column" style="padding:0 60px 0 60px;">
        <div class="columns is-centered">
          <div class="column content">
            <div class="publication-video">
                <img src="static/showcases/insert_1/results.png" alt="Edited Image" style="width: 400px; display: block; margin: auto;">
              </div>
            <p align="center">Edited Image</p>
            <p align="left">
              <b>Attack: </b> Adding new irrelevant object <i>Sheep</i> – Existence
              <br>
              <b>Question: </b> Is there a sheep in this image?
              <br>
              <b>LLaVA-1.6: </b> <b style="color: red;background:yellow">Yes, there is a sheep in this image.</b>
              It appears to be superimposed onto the scene, as it is not a typical setting for a sheep to be found.
              The sheep is sitting on the back of a purple office chair.
            </p>
            <br>
            <p align="left">
              <b>Attack: </b> Adding new irrelevant object <i>Sheep</i> with image-level caption – Existence
              <br>
              <b>Question: </b> We have an image depicting This image shows a contemporary office workspace featuring
              several desks cluttered with technological and office equipment. On the left side of the image, there is a
              vivid scene where a brown goat is digitally superimposed and sitting atop a purple office chair. This
              humorous addition contrasts with the otherwise ordinary office setting. The workspace is equipped with
              multiple monitors, laptops, office supplies like pens and staplers, and personal items. The floor is tiled,
              and the overall appearance suggests a busy, creative, and somewhat messy environment typical of tech or
              design-oriented workplaces. Is there a sheep in this image?
              <br>
              <b>LLaVA-1.6: </b> <b style="color: red;background:orange">No, there is no sheep in this image.</b>
              The animal in the image is a goat, not a sheep.
            </p>
          </div>

        </div>
      </div>
      </div>
      </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
       <div class="text-content" style="text-align: left; margin-bottom: 30px;">
         <br>
         <br>
        <b class="title is-4">Abnormal Object Insertion – Spatial Relation </b>
          </div>
        <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <b>Scene: </b> <i>Cozy bedroom interior</i>
         <br>
        <b>Detected Objects: </b> <i>Blue bed comforter</i>, <i>Wooden bookshelf, books</i>, <i>Round wall mirror</i>,
          <i>Wooden bedroom dresser</i>, <i>Window with view</i>
          <br>
        <b>Inserted Object: </b> <i>Stop Sign</i>
          <br>
        <b>Image Manipulation: </b> <i>Stop Sign</i> is inserted to the image.
        </div>
        </div>
      </div>
      </div>
    </div>

  <div class="container is-max-desktop" style="max-width: 1200px; margin: auto;">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" style="padding:0 60px 0 60px;">
        <div class="content">
          <div class="publication-video">
            <img src="static/showcases/insert_2/init.png" alt="Original Image" style="width: 400px; display: block; margin: auto;">
          </div>
          <p align="center">Original Image</p>
          <p align="left">
            <b>Attack: </b> Determine spatial relation between the irrelevant object <i>Stop Sign</i>>
            and the existing object: <i>Round wall mirror</i>.
            <br>
            <b>Question: </b> Is the stop sign on the right of a/an Round wall mirror. in this image, given their center
            positions? Where Round wall mirror. is the one that circular mirror with a simple wooden frame, reflecting a room.
            <br>
            <b>GPT-4V: </b> In the image provided, the stop sign is edited into the scene and is located centrally on
            the windowpane. The round wall mirror, described as a circular mirror with a simple wooden frame, is on the
            left side of the room above a dresser. <b style="color: red;background:lightpink"> Based on their
            positions, the stop sign is not on the right of the round wall mirror. It is instead to the left of the
            mirror when viewed from the perspective of someone standing in front of the bed facing the window
            and the dresser.</b>
          </p>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column" style="padding:0 60px 0 60px;">
        <div class="columns is-centered">
          <div class="column content">
            <div class="publication-video">
                <img src="static/showcases/insert_2/results.png" alt="Edited Image" style="width: 400px; display: block; margin: auto;">
              </div>
            <p align="center">Edited Image</p>
            <p align="left">
              <b>Attack: </b> Determine spatial relation between the irrelevant object <i>Stop Sign</i>>
              and the existing object: <i>Round wall mirror</i>.
              <br>
              <b>Question: </b> Is the stop sign on the right of a/an Round wall mirror. in this image, given their center
              positions? Where Round wall mirror. is the one that circular mirror with a simple wooden frame, reflecting a room.
              <br>
              <b>LLaVA-1.6: </b> Yes, the stop sign is on the right of the Round wall mirror. The mirror is located
              to the left of the stop sign in the image.
            </p>
          </div>

        </div>
      </div>
      </div>
      </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wu2024safety,
  title={On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities},
  author={Wu, Xiyang and Xian, Ruiqi and Guan, Tianrui and Liang, Jing and Chakraborty, Souradip and Liu, Fuxiao and Sadler, Brian and Manocha, Dinesh and Bedi, Amrit Singh},
  journal={arXiv preprint arXiv:2402.10340},
  year={2024}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>